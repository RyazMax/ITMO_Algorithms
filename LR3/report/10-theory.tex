\section*{Theory}

Optimization methods are the methods for finding optimal (in some sense) sollutions
for mathematical models which are usually expressed via an objective function $f = f(x)$, where $x$ is generally a multidimensional vector.

To solve the optimization problem $f(x) \rightarrow min_{x\in Q}$ means to find $x^{*} \in Q$ where $Q$ is the region of acceptability, such  that $f$ reaches the minimal value at $x^{*}$.

For unconstrained optimization methods (which are used in this work) the region of acceptability $Q$ is not defined.

\subsection*{First-order methods}
\subsubsection*{Gradient descent}
Gradient descent is based on the observation that if $f$
is differentiable at $a$, then $f(x)$ decreases fastest in a neighbourhood of $a$ in the direction of $-\nabla f(a)$. One may write down the following formula:
\begin{equation*}
    a_{n+1} = a_n - \beta_n \nabla f(a_n), \beta_n > 0, n = 0, 1, ...
\end{equation*}
starting with some initial approximation $a_0$.


\subsection*{Conjugate Gradient method}
Given a differentiable function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ and an initial approximation $a_0$, one starts in the steepest descent direction:

\begin{equation*}
    \Delta a_0 = - \nabla f(a_0)
\end{equation*}

Find the step size $\alpha_0 = arg min_{\alpha} f (a_0 + \alpha \Delta a_0)$ and $a_1 = a_0 + \alpha_0 \Delta a_0$. After this iteration, the following steps with
$n = 1, 2, ..$ constitue one iteration of moving along a subsequent conjuaget direction $s_n$, where $s_0 = \Delta a_0$:

\begin{itemize}
    \item Calculate the steepest direction $\Delta a_n = - \nabla f(a_n)$.
    \item Compute $\beta_n$ according to certain formulas.
    \item Update the conjugate direction $s_n = \Delta a_n + \beta_n s_{n - 1}$.
    \item Find $\alpha_n = arg min_{\alpha} f(a_n + \alpha s_n)$.
    \item Update the position: $a_{n + 1} = a_n + \alpha_n s_n$.
\end{itemize}

\subsection*{Second-order methods}
\subsubsection*{Newton's method}
Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be convex and twice differentiable. Find a root of $f'$ by
constructing a sequence $a_n$ from an initial approximation $a_0$ so that $a_n \rightarrow x^{*}$ as $n \rightarrow \infty$, where $f'(x^{*}) = 0$.

From the Taylor expansion of $f$ near $a_n$,

\begin{equation*}
    f(a_n + \Delta a) = T_f(\Delta a)= f(a_n) + f'(a_n)\Delta a + \frac{1}{2}f''(a_n)(\Delta a)^2
\end{equation*}

We use this quadratic function (with respect to $\Delta a$) as an approximant to f in a neighbourhood of $a_n$.
The vertex of the corresponding parabola gives us the next point $a_{n + 1}$. To find the vertex x-coordinate, we write

\begin{equation*}
    \Delta a = - \frac{f'(a_n)}{f''(a_n)}
\end{equation*}

\subsection*{Levenberg-Marquardt algorithm (LMA)}

The LMA is a pseudo-second order method, its application is to solve non-linear least squares problems.

For a set $(x_i, y_i)^{m}_{i=1}$, find a column-vector $\beta^{*}$ of the parameters $\beta = (\beta_1, ..., \beta_n)^T$ of the function $f(x, \beta)$ such that

\begin{equation*}
    \beta^{*} = arg min_{\beta} S(\beta), S(\beta) = \Sigma_{i=1}^m (y_i - f(x_i, \beta))^2
\end{equation*}

Start with an initial guess for $\beta$. At each iteration, $\beta$ is replaced  $\beta + \Delta \beta$. To determine $\Delta \beta$, $f(x_i, \beta)$ is approximated by its linearization:
\begin{equation*}
    f(x_i, \beta + \Delta \beta) \approx f(x_i, \beta) + J_i \Delta \beta
\end{equation*}

\begin{equation*}
    J_i = (\frac{\partial f(x_i, \beta_j)}{\partial \beta_j})_{j=1}^n
\end{equation*}

$J_i$ is a row-vector.

The sum $S(\beta)$ has its minimum at a zero gradient with respect to $\beta$. The above-mentioned linear approximation of $f(x_i, \beta + \Delta \beta)$ gives

\begin{equation*}
    S(\beta + \Delta \beta) \approx \Sigma_{i=1}^m (y_i - f(x_i, \beta) - J_i \Delta \beta)^2
\end{equation*}

or in a vector notation,

\begin{equation*}
    S(\beta + \Delta \beta) \approx (y - f(\beta))^T(y - f(\beta)) - 2(y - f(\beta))^TJ\Delta \beta + \Delta \beta^TJ^TJ\Delta\beta
\end{equation*}

where $J$ is the Jacobian matrix, whose $i$-th row equals $J_i$, and where $f(\beta)$, $y$ and $\beta$ are column-vectors with $i$-th component $f(x_i, \beta), y_i, \beta_i$ respectively.

Taking the derivative of $S(\beta + \Delta \beta)$ with respect to $\Delta \beta$ and setting to zero gives

\begin{equation*}
    (J^T J)\Delta \beta = J^T(y - f(\beta))
\end{equation*}

that is in fact a system of linear equations with respect to $\Delta \beta$.

The system may be replaced by the following damped version:

\begin{equation*}
    (J^T J + \lambda I)\Delta \beta = J^T(y - f(\beta))
\end{equation*}

where $I$ is the identity matrix, giving the increment $\Delta \beta$ to the estimated parameter vector $\beta$. The damping factor $\lambda$ is adjusted at each iteration and should be chosen to guarantee the method's convergence.